{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0084f2a3238f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#https://towardsdatascience.com/web-scraping-using-selenium-and-beautifulsoup-99195cd70a58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE doctype html>\n",
      "<html lang=\"en\"><head><meta charset=\"utf-8\"/><title>Gilbert Tanner</title><base href=\"/\"/><meta content=\"width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1\" name=\"viewport\"><link href=\"favicon.ico\" rel=\"icon\" type=\"image/x-icon\"/><link href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-okaidia.css\" rel=\"stylesheet\"/><link crossorigin=\"anonymous\" href=\"https://use.fontawesome.com/releases/v5.0.13/css/all.css\" integrity=\"sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp\" rel=\"stylesheet\"/><link crossorigin=\"anonymous\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css\" integrity=\"sha384-9gVQ4dYFwwWSjIDZnLEWnxCjeSWFphJiwGPXr1jddIhOegiu1FwO5qRGvFXOdJZ4\" rel=\"stylesheet\"/><link href=\"https://fonts.googleapis.com/css?family=Oswald\" rel=\"stylesheet\"/><script crossorigin=\"anonymous\" integrity=\"sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=\" src=\"https://code.jquery.com/jquery-3.3.1.min.js\"></script><script crossorigin=\"anonymous\" integrity=\"sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ\" src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js\"></script><script crossorigin=\"anonymous\" integrity=\"sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm\" src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js\"></script><link href=\"styles.e12ea86aec25719ed04b.bundle.css\" rel=\"stylesheet\"/></meta></head><body><app-root></app-root><script src=\"inline.318b50c57b4eba3d437b.bundle.js\" type=\"text/javascript\"></script><script src=\"polyfills.fa62713060e7012f88ea.bundle.js\" type=\"text/javascript\"></script><script src=\"main.5cfcfbc297850bba435f.bundle.js\" type=\"text/javascript\"></script></body></html>\n"
     ]
    }
   ],
   "source": [
    "videos_url = \"https://programmingwithgilbert.firebaseapp.com/videos/keras-tutorials\"\n",
    "page = urllib.request.urlopen(videos_url)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website urls\n",
    "base_url = \"https://programmingwithgilbert.firebaseapp.com/\"\n",
    "videos_url = \"https://programmingwithgilbert.firebaseapp.com/videos/keras-tutorials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Testing the driver to open Chrome\n",
    "#import os   \n",
    "#os.getcwd()\n",
    "driver = webdriver.Chrome('/Users/ndelzingaro/git_dir/analytics/Python_Web_scrape/chromedriver')  # Optional argument, if not specified will search path.\n",
    "driver.get('http://www.google.com/xhtml');\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google session\n",
    "driver = webdriver.Chrome('/Users/ndelzingaro/git_dir/analytics/Python_Web_scrape/chromedriver')\n",
    "driver.get(videos_url)\n",
    "driver.implicitly_wait(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#Finding individual pages\n",
    "num_links = len(driver.find_elements_by_link_text('Watch'))\n",
    "print(num_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical ', 'def getData(): ', 'def getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28     ', '    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10) ', '    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1) ', '    plt.imshow(X_train[0][:,:,0])\\n    plt.show() ', '    return X_train, y_train, X_test, y_test\\ngetData() ', 'import numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ngetData() Copy'], ['import numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ngetData() ', 'X_train, y_train, X_test, y_test = getData()Copy', 'from keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os ', 'def trainModel(X_train, y_train, X_test, y_test):Copy', '    batch_size = 64\\n    epochs = 15 ', '    model = Sequential()\\nCopy', \"    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) \", \"    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25)) Copy\", '    model.add(Flatten())', \"    model.add(Dense(256, activation='relu'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation='softmax')) Copy\", \"    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) \", '    datagen = ImageDataGenerator(\\n           rotation_range=10,\\n           zoom_range=0.1,\\n           width_shift_range=0.1,\\n           height_shift_range=0.1) Copy', \"    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                                       validation_data=(X_test, y_test), verbose=2,\\n                                                       steps_per_epoch=X_train.shape[0]//batch_size, \\n                                                       callbacks=[learning_rate_reduction]) \", \"    model_json = model.to_json()\\n    with open('model.json', 'w') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights('mnist_model.h5')\\n    return model Copy\", 'def loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model ', \"if(not os.path.exists('mnist_model.h5')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print('trained model')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print('loaded model')\\n    print(model.summary()) Copy\", 'import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom keras.datasets import mnist\\nfrom keras.utils import to_categorical\\nfrom keras.models import Sequential, model_from_json\\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom keras.optimizers import RMSprop\\nfrom keras.callbacks import ReduceLROnPlateau\\nimport os\\ndef getData():\\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\\n    img_rows, img_cols = 28, 28\\n    y_train = to_categorical(y_train, num_classes=10)\\n    y_test = to_categorical(y_test, num_classes=10)\\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\\n    plt.imshow(X_train[0][:,:,0])\\n    plt.show()\\n    return X_train, y_train, X_test, y_test\\n\\ndef trainModel(X_train, y_train, X_test, y_test):\\n    # training parameters\\n    batch_size = 64\\n    epochs = 15\\n    # create model and add layers\\n    model = Sequential()\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\', input_shape=(28,28,1)))\\n    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\\'relu\\'))\\n    model.add(MaxPool2D(pool_size=(2, 2)))\\n    model.add(Dropout(rate=0.25))\\n    model.add(Flatten())\\n    model.add(Dense(256, activation=\\'relu\\'))\\n    model.add(Dropout(rate=0.5))\\n    model.add(Dense(10, activation=\\'softmax\\'))\\n    # define model optimizer and callback function\\n    optimizer = RMSprop(lr=0.001)\\n    learning_rate_reduction = ReduceLROnPlateau(monitor=\\'val_acc\\', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\\n    # Image Augmentation\\n    datagen = ImageDataGenerator(\\n             rotation_range=10,\\n             zoom_range=0.1,\\n             width_shift_range=0.1,\\n             height_shift_range=0.1)\\n     # compile model define loss, optimizer and metrics\\n    model.compile(loss=\\'categorical_crossentropy\\', optimizer=optimizer, metrics=[\\'accuracy\\'])\\n     # Train model\\n    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\\n                                   validation_data=(X_test, y_test), verbose=2, \\n \\t\\t\\t\\t  steps_per_epoch=X_train.shape[0]//batch_size, \\n \\t\\t\\t\\t  callbacks=[learning_rate_reduction])\\n     # Save model structure and weights\\n    model_json = model.to_json()\\n    with open(\\'model.json\\', \\'w\\') as json_file:\\n        json_file.write(model_json)\\n    model.save_weights(\\'mnist_model.h5\\')\\n    return model\\n\\ndef loadModel():\\n    json_file = open(\\'model.json\\', \\'r\\')\\n    model_json = json_file.read()\\n    json_file.close()\\n    model = model_from_json(model_json)\\n    model.load_weights(\"mnist_model.h5\")\\n    return model\\n\\nX_train, y_train, X_test, y_test = getData()\\n\\nif(not os.path.exists(\\'mnist_model.h5\\')):\\n    model = trainModel(X_train, y_train, X_test, y_test)\\n    print(\\'trained model\\')\\n    print(model.summary())\\nelse:\\n    model = loadModel()\\n    print(\\'loaded model\\')\\n    print(model.summary())\\n '], ['from tkinter import *\\nfrom PIL import ImageGrab\\nimport numpy as np\\nimport scipy.misc\\nfrom keras.models import Sequential, model_from_json\\nimport os\\nfrom keras_cnn import trainModel, getData\\n ', 'class Paint(object):Copy', '    def __init__(self):\\n        self.root = Tk() ', \"        self.c = Canvas(self.root, bg='white', width=280, height=280)\\n        self.c.grid(row=1, columnspan=5)\\n\\n        self.classify_button = Button(self.root, text='classify', command=lambda:self.classify(self.c))\\n        self.classify_button.grid(row=0, column=1)\\n\\n        self.clear = Button(self.root, text='clear', command=self.clear)\\n        self.clear.grid(row=0, column=3)\\n\\n        self.prediction_text = Text(self.root, height=2, width=10)\\n        self.prediction_text.grid(row=2, column=3)\\n Copy\", \"    def setup(self):\\n        self.old_x = None\\n        self.old_y = None\\n        self.line_width = 15\\n        self.color = 'black'\\n        self.c.bind('', self.paint)\\n        self.c.bind('', self.reset)\\n \", '        self.setup()\\n        self.root.mainloop() Copy', 'def paint(self, event):\\n        paint_color = self.color\\n        if self.old_x and self.old_y:\\n            self.c.create_line(self.old_x, self.old_y, event.x, event.y,\\n                               width=self.line_width, fill=paint_color,\\n                               capstyle=ROUND, smooth=TRUE, splinesteps=36)\\n        self.old_x = event.x\\n        self.old_y = event.y\\n ', '    def clear(self):\\n        self.c.delete(\"all\") Copy', '    def reset(self, event):\\n        self.old_x, self.old_y = None, None ', '    def classify(self, widget):Copy', '        x=self.root.winfo_rootx()+widget.winfo_x()\\n        y=self.root.winfo_rooty()+widget.winfo_y()\\n        x1=x+widget.winfo_width()\\n        y1=y+widget.winfo_height() ', \"        ImageGrab.grab().crop((x,y,x1,y1)).resize((28, 28)).save('classify.png')\\n        img = scipy.misc.imread('classify.png', flatten=False, mode='P')\\n        img = np.array(img)\\n        img = np.reshape(img, (1, 28, 28, 1))\\n Copy\", '        img[img==0] = 255\\n        img[img==225] = 0 ', '        pred = self.model.predict([img])\\n        pred = np.argmax(pred)\\n        print(pred)\\n        self.prediction_text.delete(\"1.0\", END)\\n        self.prediction_text.insert(END, pred)\\n Copy', '    def loadModel(self):\\n        # if model exists load it else create and train it\\n        if(os.path.exists(\\'mnist_model.h5\\')):\\n            print(\\'loading model\\')\\n            json_file = open(\\'model.json\\', \\'r\\')\\n            model_json = json_file.read()\\n            json_file.close()\\n            model = model_from_json(model_json)\\n            model.load_weights(\"mnist_model.h5\")\\n            return model\\n        else:\\n            print(\\'train model\\')\\n            X_train, y_train, X_test, y_test = getData()\\n            model = trainModel(X_train, y_train, X_test, y_test)\\n \\t   return model\\n ', '        self.model = self.loadModel()Copy', \"if __name__ == '__main__':\\n    Paint() \", 'from tkinter import *\\nfrom PIL import ImageGrab\\nimport numpy as np\\nimport scipy.misc\\nfrom keras.models import Sequential, model_from_json\\nimport os\\nfrom keras_cnn import trainModel, getData\\nclass Paint(object):\\n\\n    def __init__(self):\\n        self.root = Tk()\\n\\n        #defining Canvas\\n        self.c = Canvas(self.root, bg=\\'white\\', width=280, height=280)\\n        self.c.grid(row=1, columnspan=5)\\n\\n        self.classify_button = Button(self.root, text=\\'classify\\', command=lambda:self.classify(self.c))\\n        self.classify_button.grid(row=0, column=1)\\n\\n        self.clear = Button(self.root, text=\\'clear\\', command=self.clear)\\n        self.clear.grid(row=0, column=3)\\n\\n        self.prediction_text = Text(self.root, height=2, width=10)\\n        self.prediction_text.grid(row=2, column=3)\\n\\n        self.model = self.loadModel()\\n        self.setup()\\n        self.root.mainloop()\\n\\n    def setup(self):\\n        self.old_x = None\\n        self.old_y = None\\n        self.line_width = 15\\n        self.color = \\'black\\'\\n        self.c.bind(\\'\\', self.paint)\\n        self.c.bind(\\'\\', self.reset)\\n\\n    def clear(self):\\n        self.c.delete(\"all\")\\n\\n    def paint(self, event):\\n        paint_color = self.color\\n        if self.old_x and self.old_y:\\n            self.c.create_line(self.old_x, self.old_y, event.x, event.y,\\n                               width=self.line_width, fill=paint_color,\\n                               capstyle=ROUND, smooth=TRUE, splinesteps=36)\\n        self.old_x = event.x\\n        self.old_y = event.y\\n\\n    def reset(self, event):\\n        self.old_x, self.old_y = None, None\\n\\n    def classify(self, widget):\\n        #getting pixel information\\n        x=self.root.winfo_rootx()+widget.winfo_x()\\n        y=self.root.winfo_rooty()+widget.winfo_y()\\n        x1=x+widget.winfo_width()\\n        y1=y+widget.winfo_height()\\n        #save drawing\\n        ImageGrab.grab().crop((x,y,x1,y1)).resize((28, 28)).save(\\'classify.png\\')\\n        img = scipy.misc.imread(\\'classify.png\\', flatten=False, mode=\\'P\\')\\n        img = np.array(img)\\n        img = np.reshape(img, (1, 28, 28, 1))\\n        # Change pixels to work with our classifier\\n        img[img==0] = 255\\n        img[img==225] = 0\\n        # Predict digit\\n        pred = self.model.predict([img])\\n        # Get index with highest probability\\n        pred = np.argmax(pred)\\n        print(pred)\\n        self.prediction_text.delete(\"1.0\", END)\\n        self.prediction_text.insert(END, pred)\\n\\n    def loadModel(self):\\n        # if model exists load it else create and train it\\n        if(os.path.exists(\\'mnist_model.h5\\')):\\n            print(\\'loading model\\')\\n            json_file = open(\\'model.json\\', \\'r\\')\\n            model_json = json_file.read()\\n            json_file.close()\\n            model = model_from_json(model_json)\\n            model.load_weights(\"mnist_model.h5\")\\n            return model\\n        else:\\n            print(\\'train model\\')\\n            X_train, y_train, X_test, y_test = getData()\\n            model = trainModel(X_train, y_train, X_test, y_test)\\n \\t   return model\\n\\nif __name__ == \\'__main__\\':\\n    Paint() Copy'], ['from __future__ import print_function\\nfrom keras.callbacks import LambdaCallback\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation\\nfrom keras.layers import LSTM\\nfrom keras.optimizers import RMSprop\\nimport numpy as np\\nimport random\\nimport sys\\nimport io ', \"text = open('sherlock_homes.txt', 'r').read().lower()\\nprint('text length', len(text))Copy\", \"chars = sorted(list(set(text)))\\nprint('total chars: ', len(chars))\", 'char_indices = dict((c, i) for i, c in enumerate(chars))\\nindices_char = dict((i, c) for i, c in enumerate(chars))Copy', \"maxlen = 40\\nstep = 3\\nsentences = []\\nnext_chars = []\\nfor i in range(0, len(text) - maxlen, step):\\n    sentences.append(text[i: i + maxlen])\\n    next_chars.append(text[i + maxlen])\\nprint('nb sequences:', len(sentences))\", 'x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\\nfor i, sentence in enumerate(sentences):\\n    for t, char in enumerate(sentence):\\n        x[i, t, char_indices[char]] = 1\\n    y[i, char_indices[next_chars[i]]] = 1 Copy', \"model = Sequential()\\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\\nmodel.add(Dense(len(chars)))\\nmodel.add(Activation('softmax'))\\n\", \"from keras.layers import Dropout\\nmodel = Sequential()\\nmodel.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\\nmodel.add(Dropout(0.25))\\nmodel.add(LSTM(256))\\nmodel.add(Dropout(0.25))\\nmodel.add(Dense(len(chars)))\\nmodel.add(Activation('softmax')) Copy\", \"optimizer = RMSprop(lr=0.01)\\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\", \"def sample(preds, temperature=1.0):\\n    # helper function to sample an index from a probability array\\n    preds = np.asarray(preds).astype('float64')\\n    preds = np.log(preds) / temperature\\n    exp_preds = np.exp(preds)\\n    preds = exp_preds / np.sum(exp_preds)\\n    probas = np.random.multinomial(1, preds, 1)\\n    return np.argmax(probas)Copy\", 'def on_epoch_end(epoch, logs):\\n    # Function invoked at end of each epoch. Prints generated text.\\n    print()\\n    print(\\'----- Generating text after Epoch: %d\\' % epoch)\\n\\n    start_index = random.randint(0, len(text) - maxlen - 1)\\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\\n        print(\\'----- diversity:\\', diversity)\\n\\n        generated = \\'\\'\\n        sentence = text[start_index: start_index + maxlen]\\n        generated += sentence\\n        print(\\'----- Generating with seed: \"\\' + sentence + \\'\"\\')\\n        sys.stdout.write(generated)\\n\\n        for i in range(400):\\n            x_pred = np.zeros((1, maxlen, len(chars)))\\n            for t, char in enumerate(sentence):\\n                x_pred[0, t, char_indices[char]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_char = indices_char[next_index]\\n\\n            generated += next_char\\n            sentence = sentence[1:] + next_char\\n\\n            sys.stdout.write(next_char)\\n            sys.stdout.flush()\\n        print()', 'print_callback = LambdaCallback(on_epoch_end=on_epoch_end)Copy', 'from keras.callbacks import ModelCheckpoint\\n\\nfilepath = \"weights.hdf5\"\\ncheckpoint = ModelCheckpoint(filepath, monitor=\\'loss\\',\\nverbose=1, save_best_only=True,\\nmode=\\'min\\')', \"from keras.callbacks import ReduceLROnPlateau\\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\\n                               patience=1, min_lr=0.001)Copy\", 'callbacks = [print_callback, checkpoint, reduce_lr]', 'model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)Copy', \"def generate_text(length, diversity):\\n    # Get random starting text\\n    start_index = random.randint(0, len(text) - maxlen - 1)\\n    generated = ''\\n    sentence = text[start_index: start_index + maxlen]\\n    generated += sentence\\n    for i in range(length):\\n            x_pred = np.zeros((1, maxlen, len(chars)))\\n            for t, char in enumerate(sentence):\\n                x_pred[0, t, char_indices[char]] = 1.\\n\\n            preds = model.predict(x_pred, verbose=0)[0]\\n            next_index = sample(preds, diversity)\\n            next_char = indices_char[next_index]\\n\\n            generated += next_char\\n            sentence = sentence[1:] + next_char\\n    return generated\", 'print(generate_text(500, 0.2))Copy'], ['import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom keras.models import Sequential, load_model\\nfrom keras.layers import LSTM, Dense, Dropout\\nimport os ', \"df = pd.read_csv('AAPL_data.csv')\\ndf.head() Copy\", \"df = df['open'].values\\ndf = df.reshape(-1, 1)\\nprint(df.shape)\\ndf[:5] \", 'dataset_train = np.array(df[:int(df.shape[0]*0.8)])\\ndataset_test = np.array(df[int(df.shape[0]*0.8)-50:])\\nprint(dataset_train.shape)\\nprint(dataset_test.shape) Copy', 'scaler = MinMaxScaler(feature_range=(0,1))\\ndataset_train = scaler.fit_transform(dataset_train)\\ndataset_train[:5] ', 'dataset_test = scaler.transform(dataset_test) dataset_test[:5]Copy', 'def create_dataset(df):\\n    x = []\\n    y = []\\n    for i in range(50, df.shape[0]):\\n        x.append(df[i-50:i, 0])\\n        y.append(df[i, 0])\\n    x = np.array(x)\\n    y = np.array(y)\\n    return x,y ', ' x_train, y_train = create_dataset(dataset_train)\\nx_train[:1] Copy', 'x_test, y_test = create_dataset(dataset_test)\\nx_test[:1] ', '# Reshape features for LSTM Layer\\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)) Copy', 'model = Sequential()\\nmodel.add(LSTM(units=96, return_sequences=True, input_shape=(x_train.shape[1], 1)))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96, return_sequences=True))\\nmodel.add(Dropout(0.2))\\nmodel.add(LSTM(units=96))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(units=1)) ', \"model.compile(loss='mean_squared_error', optimizer='adam')Copy\", \"if(not os.path.exists('stock_prediction.h5')):\\n    model.fit(x_train, y_train, epochs=50, batch_size=32)\\n    model.save('stock_prediction.h5')\\n \", \"model = load_model('stock_prediction.h5') Copy\", 'predictions = model.predict(x_test)\\npredictions = scaler.inverse_transform(predictions)\\n\\nfig, ax = plt.subplots(figsize=(8,4))\\nplt.plot(df, color=\\'red\\',  label=\"True Price\")\\nax.plot(range(len(y_train)+50,len(y_train)+50+len(predictions)),\\npredictions, color=\\'blue\\', label=\\'Predicted Testing Price\\')\\nplt.legend() ', \"y_test_scaled = scaler.inverse_transform(y_test.reshape(-1, 1))\\n\\nfig, ax = plt.subplots(figsize=(8,4))\\nax.plot(y_test_scaled, color='red', label='True Testing Price')\\nplt.plot(predictions, color='blue', label='Predicted Testing Price')\\nplt.legend() Copy\"], ['import re\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nimport matplotlib.pyplot as plt\\n\\nfrom keras.models import Sequential, load_model\\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras.preprocessing.sequence import pad_sequences ', \"data = pd.read_csv('Tweets.csv')\\ndata = data.sample(frac=1).reset_index(drop=True)\\nprint(data.shape)\\ndata.head() Copy\", \"data = data[['airline_sentiment', 'text']]\\ndata.head() \", \"data['airline_sentiment'].value_counts().sort_index().plot.bar() Copy\", \"data['text'].str.len().plot.hist() \", \"data['text'].apply(lambda x: x.lower())\\n#transform text to lowercase\\ndata['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-z0-9\\\\s]', '', x))\\ndata['text'].head() Copy\", 'tokenizer = Tokenizer(num_words=5000, split=\" \")\\ntokenizer.fit_on_texts(data[\\'text\\'].values)\\n\\nX = tokenizer.texts_to_sequences(data[\\'text\\'].values)\\nX = pad_sequences(X) # padding our text vector so they all have the same length\\nX[:5] ', \"model = Sequential()\\nmodel.add(Embedding(5000, 256, input_length=X.shape[1]))\\nmodel.add(Dropout(0.3))\\nmodel.add(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\\nmodel.add(LSTM(256, dropout=0.3, recurrent_dropout=0.2))\\nmodel.add(Dense(3, activation='softmax'))\\n Copy\", \"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel.summary() \", \"y = pd.get_dummies(data['airline_sentiment']).values\\n[print(data['airline_sentiment'][i], y[i]) for i in range(0,5)] Copy\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) ', 'batch_size = 32\\nepochs = 8\\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2) Copy', \"model.save('sentiment_analysis.h5')\", \"predictions = model.predict(X_test)\\n[print(data['text'][i], predictions[i], y_test[i]) for i in range(0, 5)] Copy\", \"pos_count, neu_count, neg_count = 0, 0, 0\\nreal_pos, real_neu, real_neg = 0, 0, 0\\nfor i, prediction in enumerate(predictions):\\n    if np.argmax(prediction)==2:\\n        pos_count += 1\\n    elif np.argmax(prediction)==1:\\n        neu_count += 1\\n    else:\\n        neg_count += 1\\n\\n        if np.argmax(y_test[i])==2:\\n        real_pos += 1\\n    elif np.argmax(y_test[i])==1:\\n        real_neu += 1\\n    else:\\n        real_neg +=1\\n\\nprint('Positive predictions:', pos_count)\\nprint('Neutral predictions:', neu_count)\\nprint('Negative predictions:', neg_count)\\nprint('Real positive:', real_pos)\\nprint('Real neutral:', real_neu)\\nprint('Real negative:', real_neg) \"], ['import numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, LSTM\\nfrom keras.models import Model\\nfrom keras.datasets import mnist ', \"(x_train, y_train), (x_test, y_test) = mnist.load_data()\\ndel y_train\\ndel y_test\\n\\n# scale data between 0 and 1\\nx_train = x_train.astype('float32') / 256\\nx_test = x_test.astype('float32') / 256\\n\\n# Reshape data\\nx_train = x_train.reshape(len(x_train), 28*28)\\nx_test = x_test.reshape(len(x_test), 28*28)\\n\\nprint('Training data shape:', x_train.shape)\\nprint('Testing data shape:', x_test.shape) Copy\", \"input_layer = Input(shape=(784,))\\n# encoded layer. compresses data by the factor 12.25 (784/64=12.25)\\nencoded = Dense(64, activation='relu')(input_layer)\\n# decoded layer. reconstructs the input (units are equal to the input dimension of the image)\\ndecoded = Dense(784, activation='sigmoid')(encoded) \", '# this model maps an input to its reconstruction\\nautoencoder = Model(input_layer, decoded) Copy', '# this model maps an input to its encoded representation\\nencoder = Model(input_layer, encoded) ', '# for the decoder we need to create an input layer which has the dimensionality of the encoded layer\\ninput_layer_dec = Input(shape=(64,))\\n# retrieve the last layer of the autoencoder model and place it after the input layer\\ndecoder_layer = autoencoder.layers[-1](input_layer_dec)\\n\\ndecoder = Model(input_layer_dec, decoder_layer) Copy', \"autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\\n\\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=256,\\n                shuffle=True,validation_data=(x_test, x_test)) \", 'encoded_images = encoder.predict(x_test)\\ndecoded_images = decoder.predict(encoded_images) # takes the output from the encoder as input Copy', 'def visualize_data(data, size):\\n    n = 10 # number of digits that will be displayed\\n    plt.figure(figsize=(20, 4))\\n    plt.gray()\\n    for i in range(n):\\n        ax = plt.subplot(2, n, i+1)\\n        plt.imshow(data[i].reshape(size, size)) # reshape the image back to its normal shape\\n        # disable axis\\n        ax.get_xaxis().set_visible(False)\\n        ax.get_yaxis().set_visible(False)\\n    plt.show()     ', 'visualize_data(x_test, 28)\\nvisualize_data(decoded_images, 28) Copy', 'visualize_data(encoded_images, 8) ', \"input_layer = Input(shape=(784,))\\n encoded = Dense(128, activation='relu')(input_layer)\\nencoded = Dense(64, activation='relu')(encoded)\\n\\ndecoded = Dense(128, activation='relu')(encoded)\\ndecoded = Dense(784, activation='sigmoid')(decoded) Copy\", \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\\n\\nautoencoder.fit(x_train, x_train, epochs=100, batch_size=256,\\n                shuffle=True,validation_data=(x_test, x_test)) \", 'autoencoded_images = autoencoder.predict(x_test) Copy', 'visualize_data(x_test, 28)\\nvisualize_data(autoencoded_images, 28) ', \"# Reshape data for Convolutional Layer\\nx_train = x_train.reshape(len(x_train), 28, 28, 1)\\nx_test = x_test.reshape(len(x_test), 28, 28, 1)\\nprint('Training data shape:', x_train.shape)\\nprint('Testing data shape:', x_test.shape) Copy\", \"input_layer = Input(shape=(28, 28, 1))\\n\\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nencoded = MaxPooling2D((2,2), padding='same')(x)\\n\\n# encoded representation is (4, 4, 8) i.e. 128-dimensional\\n\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(16, (3, 3), activation='relu')(x)\\nx = UpSampling2D((2, 2))(x)\\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) \", \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(loss='binary_crossentropy', optimizer='adadelta') Copy\", 'autoencoder.fit(x_train, x_train, epochs=50,\\n                batch_size=128, shuffle=True,validation_data=(x_test, x_test)) ', 'autoencoded_images = autoencoder.predict(x_test)Copy', 'visualize_data(x_test, 28)\\nvisualize_data(autoencoded_images, 28) ', 'noise_factor = 0.5\\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\\n\\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\\nx_test_noisy = np.clip(x_test_noisy, 0., 1.) Copy', 'visualize_data(x_test_noisy, 28)', \"input_layer = Input(shape=(28, 28, 1))\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\\nx = MaxPooling2D((2, 2), padding='same')(x)\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\\nencoded = MaxPooling2D((2, 2), padding='same')(x)\\n\\n# at this point the representation is (7, 7, 32)\\n\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\\nx = UpSampling2D((2, 2))(x)\\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\\nx = UpSampling2D((2, 2))(x)\\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) Copy\", \"autoencoder = Model(input_layer, decoded)\\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') \", 'autoencoder.fit(x_train_noisy, x_train, epochs=100,batch_size=128,\\n                shuffle=True,validation_data=(x_test_noisy, x_test))Copy', 'denoised_data = autoencoder.predict(x_train_noisy)\\nvisualize_data(x_train_noisy, 28)\\nvisualize_data(denoised_data, 28) '], [\"import pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom keras.utils import to_categorical\\n\\nnp.random.seed(0)\\n\\niris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\\n                   names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\\nle = LabelEncoder()\\niris['label'] = le.fit_transform(iris['label'])\\nX = np.array(iris.drop(['label'], axis=1))\\ny = np.array(iris['label'])\\ny = to_categorical(y, num_classes=3)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\\niris.head() \", \"from keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\n\\ndef c_model():\\n    model = Sequential()\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(16, activation='relu'))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32) Copy\", 'model.fit(X_train, y_train)', \"from sklearn.model_selection import GridSearchCV\\n\\nmodel = KerasClassifier(build_fn=c_model)\\n\\nbatch_sizes = [10, 20, 50, 100]\\nepochs = [5, 10, 50]\\nparameters = {'batch_size': batch_sizes, 'epochs': epochs}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) Copy\", \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \", \"def c_model(optimizer):\\n    model = Sequential()\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(16, activation='relu'))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32)\\nparameters = {'optimizer':['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) Copy\", \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \", \"def c_model(activation):\\n    model = Sequential()\\n    model.add(Dense(32, activation=activation))\\n    model.add(Dense(16, activation=activation))\\n    model.add(Dense(3, activation='softmax'))\\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n    return model\\n\\nmodel = KerasClassifier(build_fn=c_model, epochs=50, batch_size=32)\\nparameters = {'activation':['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']}\\nclf = GridSearchCV(model, parameters)\\nclf.fit(X_train, y_train) Copy\", \"print(clf.best_score_, clf.best_params_)\\nmeans = clf.cv_results_['mean_test_score']\\nparameters = clf.cv_results_['params']\\nfor mean, parammeter in zip(means, parameters):\\n    print(mean, parammeter) \"], [\"from keras.applications.resnet50 import ResNet50\\nfrom keras.preprocessing import image\\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\\nimport numpy as np\\n\\n# load model\\nmodel = ResNet50(weights='imagenet') \", \"img_path = 'elephant.jpg'\\nimg = image.load_img(img_path, target_size=(224, 224))\\nx = image.img_to_array(img)\\nx = np.expand_dims(x, axis=0)\\nx = preprocess_input(x) Copy\", \"prediction = model.predict(x)\\nprint('Predicted:', decode_predictions(prediction, top=5)[0]) \", \"from keras.models import Model\\nfrom keras.applications.vgg19 import VGG19\\n\\nbase_model = VGG19(weights='imagenet')\\nbase_model.summary() Copy\", \"model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\\nmodel.summary() \", 'block5_pool_features = model.predict(x)\\nblock5_pool_features Copy', 'from keras.layers import Flatten, Dense, Dropout\\n\\nbase_model = VGG19(weights = \"imagenet\", include_top=False, input_shape = (256, 256, 3))\\n\\nfor layer in base_model.layers[:5]:\\n    layer.trainable = False\\n\\n# Adding custom layers\\nx = base_model.output\\nx = Flatten()(x)\\n x = Dense(2048, activation=\\'relu\\')(x)\\nx = Dropout(0.3)(x)\\nx = Dense(1024, activation=\\'relu\\')(x)\\noutput = Dense(10, activation=\\'softmax\\')(x)\\n\\nmodel = Model(inputs=base_model.input, outputs=output)\\nmodel.summary() '], [\"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport os\\nimport warnings\\n\\nfrom keras.layers import Input, Embedding, Flatten, Dot, Dense\\nfrom keras.models import Model\\n\\nwarnings.filterwarnings('ignore')\\n%matplotlib inline\", \"dataset = pd.read_csv('ratings.csv')\\nprint(dataset.head())\\nprint(dataset.shape)Copy\", 'from sklearn.model_selection import train_test_split\\ntrain, test = train_test_split(dataset, test_size=0.2, random_state=42)\\nprint(train.head())\\nprint(test.head())\\nn_users = len(dataset.user_id.unique())\\nprint(n_users)\\nn_books = len(dataset.book_id.unique())\\nprint(n_books)', 'book_input = Input(shape=[1], name=\"Book-Input\")\\nbook_embedding = Embedding(n_books, 5, name=\"Book-Embedding\")(book_input)\\nbook_vec = Flatten(name=\"Flatten-Books\")(book_embedding)\\n\\nuser_input = Input(shape=[1], name=\"User-Input\")\\nuser_embedding = Embedding(n_users, 5, name=\"User-Embedding\")(user_input)\\nuser_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\\n\\nprod = Dot(name=\"Dot-Product\", axes=1)([book_vec, user_vec])\\nmodel = Model([user_input, book_input], prod)\\nmodel.compile(\\'adam\\', \\'mean_squared_error\\') Copy', 'history = model.fit([train.user_id, train.book_id], train.rating, epochs=5, verbose=1)\\nmodel.save(\\'regression_model.h5\\')\\nplt.plot(history.history[\\'loss\\'])\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Training Error\") ', \"# Extract embeddings\\nbook_em = model.get_layer('Book-Embedding')\\nbook_em_weights = book_em.get_weights()[0]\\nprint(book_em_weights[:5]) Copy\", 'from sklearn.decomposition import PCA\\nimport seaborn as sns\\n\\npca = PCA(n_components=2)\\npca_result = pca.fit_transform(book_em_weights)\\nsns.scatterplot(x=pca_result[:,0], y=pca_result[:,1]) ', 'book_em_weights = book_em_weights / np.linalg.norm(book_em_weights, axis = 1).reshape((-1, 1))\\nbook_em_weights[0][:10]\\np.sum(np.square(book_em_weights[0]))\\npca = PCA(n_components=2)\\npca_result = pca.fit_transform(book_em_weights)\\nsns.scatterplot(x=pca_result[:,0], y=pca_result[:,1]) Copy', 'from sklearn.manifold import TSNE\\n\\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\\ntnse_results = tsne.fit_transform(book_em_weights)\\nsns.scatterplot(x=tnse_results[:,0], y=tnse_results[:,1]) ', '# Creating dataset for making recommendations for the first user\\nbook_data = np.array(list(set(dataset.book_id)))\\nuser = np.array([1 for i in range(len(book_data))])\\npredictions = model.predict([user, book_data])\\npredictions = np.array([a[0] for a in predictions])\\nrecommended_book_ids = (-predictions).argsort()[:5]\\n Copy', \"books = pd.read_csv('books.csv')\\nbooks[books['id'].isin(recommended_book_ids)] \"]]\n"
     ]
    }
   ],
   "source": [
    "num_links = len(driver.find_elements_by_link_text('Watch'))\n",
    "code_blocks = []\n",
    "for i in range(num_links):\n",
    "    # navigate to link\n",
    "    button = driver.find_elements_by_class_name(\"btn-primary\")[i]\n",
    "    button.click()\n",
    "    # get soup\n",
    "    element = WebDriverWait(driver, 10).until(lambda x: x.find_element_by_id('iframe_container'))\n",
    "    tutorial_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tutorial_code_soup = tutorial_soup.find_all('div', attrs={'class': 'code-toolbar'})\n",
    "    tutorial_code = [i.getText() for i in tutorial_code_soup]\n",
    "    code_blocks.append(tutorial_code)\n",
    "    # go back to initial page\n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "print(code_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the data stored in an array we can save it to disk. \n",
    "#We will save the code from each tutorial in a separate .txt file\n",
    "for i, tutorial_code in enumerate(code_blocks):\n",
    "    with open('code_blocks{}.txt'.format(i), 'w') as f:\n",
    "        for code_block in tutorial_code:\n",
    "            f.write(code_block+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books = pd.read_csv('books.csv')\n",
      "books[books['id'].isin(recommended_book_ids)] \n"
     ]
    }
   ],
   "source": [
    "print(code_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
